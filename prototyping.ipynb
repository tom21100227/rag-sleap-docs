{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "021d6a44",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "I need to index everything for sleap and sleap-io at first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "239b08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".local.env\")\n",
    "os.environ[\"LANGSMITH_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = dotenv.get_key(\".local.env\", \"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"rag-sleap-docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec43a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import langchain_community\n",
    "import langchain_google_vertexai\n",
    "import chromadb\n",
    "import os\n",
    "import vertexai\n",
    "vertexai.init(project=dotenv.get_key(\".local.env\", \"GOOGLE_CLOUD_PROJECT_ID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222008b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, world!\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 9, 'candidates_token_count': 5, 'total_token_count': 14, 'prompt_tokens_details': [{'modality': 1, 'token_count': 9}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 5}], 'thoughts_token_count': 0, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.010825920104980468, 'model_name': 'gemini-2.0-flash-lite'}, id='run--4d62e52c-c416-4718-9bbd-0f418449d22e-0', usage_metadata={'input_tokens': 9, 'output_tokens': 5, 'total_tokens': 14, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check: at this point I should be able to see the LangSmith configuration\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "# gemini = ChatVertexAI(\n",
    "#     model_name=\"gemini-2.0-flash-lite\",\n",
    "#     temperature=0.2)\n",
    "# gemini.invoke(\"Repeat after me: 'Hello, world!'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fed1e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections created or accessed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Make or bind a ChromaDB client\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "try:\n",
    "    sleap_collection = client.get_or_create_collection(name=\"sleap\")\n",
    "    sleap_io_collection = client.get_or_create_collection(name=\"sleap_io\")\n",
    "    print(\"Collections created or accessed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating or accessing collection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efdc1a8",
   "metadata": {},
   "source": [
    "## Smart Embedding\n",
    "\n",
    "Instead of this html based method, I will separately parse the codes (with comments) with ast and generate embeddings for each function and class. This will allow me to search for specific functions or classes more effectively. For guides and examples, I will use the existing markdown files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "847ab109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing guides in: /Users/chan/PersonalProjects/rag-sleap-docs/sleap/docs...\n",
      "-> Found 31 guide files.\n",
      "Parsing source code in: /Users/chan/PersonalProjects/rag-sleap-docs/sleap/sleap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:146: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<unknown>:98: SyntaxWarning: invalid escape sequence '\\*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Found 2072 docstrings in source code.\n",
      "\n",
      "✅ Total documents collected: 2103\n",
      "\n",
      "Example documents:\n",
      "Source: sleap-guide, File: docs/CODE_OF_CONDUCT.md\n",
      "# Contributor Covenant Code of Conduct\n",
      "\n",
      "## Our Pledge\n",
      "\n",
      "In the interest of fostering an open and welc...\n",
      "----------------------------------------\n",
      "Source: sleap-guide, File: docs/help.md\n",
      "# Help\n",
      "\n",
      "Stuck? Can't get SLEAP to run? Crashing? Try the recommended tips below.\n",
      "\n",
      "## Installation\n",
      "\n",
      "#...\n",
      "----------------------------------------\n",
      "Source: sleap-api, File: sleap/rangelist.py, Object: RangeList\n",
      "class RangeList:\n",
      "\n",
      "Class for manipulating a list of range intervals.\n",
      "Each range interval in the list ...\n",
      "\n",
      "----------------------------------------\n",
      "Source: sleap-api, File: sleap/rangelist.py, Object: list\n",
      "def list(self):\n",
      "\n",
      "Returns the list of ranges....\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "from pathlib import Path\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- 1. CONFIGURE YOUR PATHS ---\n",
    "# Point this to the root of the cloned sleap repository\n",
    "REPO_PATH = Path(\"/Users/chan/PersonalProjects/rag-sleap-docs/sleap\") \n",
    "DOCS_PATH = REPO_PATH / \"docs\"\n",
    "SRC_PATH = REPO_PATH / \"sleap\"\n",
    "\n",
    "# This class uses an Abstract Syntax Tree to safely parse Python code\n",
    "class CodeParser(ast.NodeVisitor):\n",
    "    \"\"\"An AST visitor to extract functions, classes, and their docstrings.\"\"\"\n",
    "    def __init__(self, file_path: str, source: str, repo_path: Path = REPO_PATH):\n",
    "        self.file_path = file_path\n",
    "        self.source = source\n",
    "        self.documents = []\n",
    "        self.repo_path = repo_path\n",
    "\n",
    "    def visit_FunctionDef(self, node: ast.FunctionDef):\n",
    "        docstring = ast.get_docstring(node)\n",
    "        if docstring:\n",
    "            # Reconstruct a simple signature\n",
    "            signature = f\"def {node.name}({ast.unparse(node.args)}):\"\n",
    "            content = f\"{signature}\\n\\n{docstring}\"\n",
    "            self.documents.append(Document(\n",
    "                page_content=content,\n",
    "                metadata={\"source\": f\"{self.source}-api\", \"file\": self.file_path, \"object\": node.name}\n",
    "            ))\n",
    "        self.generic_visit(node) # Continue visiting children\n",
    "\n",
    "    def visit_ClassDef(self, node: ast.ClassDef):\n",
    "        docstring = ast.get_docstring(node)\n",
    "        if docstring:\n",
    "            signature = f\"class {node.name}:\"\n",
    "            content = f\"{signature}\\n\\n{docstring}\"\n",
    "            self.documents.append(Document(\n",
    "                page_content=content,\n",
    "                metadata={\"source\": f\"{self.source}-api\", \"file\": self.file_path, \"object\": node.name}\n",
    "            ))\n",
    "        self.generic_visit(node) # Continue visiting children\n",
    "\n",
    "def parse_source_code(src_path: Path, source: str, repo_path: Path = REPO_PATH) -> list[Document]:\n",
    "    \"\"\"Parses Python source files to extract API documentation.\"\"\"\n",
    "    print(f\"Parsing source code in: {src_path}...\")\n",
    "    documents = []\n",
    "    for py_file in src_path.rglob(\"*.py\"):\n",
    "        try:\n",
    "            file_content = py_file.read_text(encoding=\"utf-8\")\n",
    "            tree = ast.parse(file_content)\n",
    "            relative_path = str(py_file.relative_to(repo_path))\n",
    "            parser = CodeParser(relative_path, source, repo_path)\n",
    "            parser.visit(tree)\n",
    "            documents.extend(parser.documents)\n",
    "        except Exception as e:\n",
    "            print(f\"--> Could not parse {py_file}: {e}\")\n",
    "            \n",
    "    print(f\"-> Found {len(documents)} docstrings in source code.\")\n",
    "    return documents\n",
    "\n",
    "def parse_guides(docs_path: Path, source: str, repo_path: Path = REPO_PATH) -> list[Document]:\n",
    "    \"\"\"Parses Markdown guides, loading each file as a single document.\"\"\"\n",
    "    print(f\"Parsing guides in: {docs_path}...\")\n",
    "    documents = []\n",
    "    for md_file in docs_path.rglob(\"*.md\"):\n",
    "        file_content = md_file.read_text(encoding=\"utf-8\")\n",
    "        relative_path = str(md_file.relative_to(repo_path))\n",
    "        doc = Document(\n",
    "            page_content=file_content,\n",
    "            metadata={\"source\": f\"{source}-guide\", \"file\": relative_path}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    for rst_file in docs_path.rglob(\"*.rst\"):\n",
    "        file_content = rst_file.read_text(encoding=\"utf-8\")\n",
    "        relative_path = str(rst_file.relative_to(REPO_PATH))\n",
    "        doc = Document(\n",
    "            page_content=file_content,\n",
    "            metadata={\"source\": f\"{source}-guide\", \"file\": relative_path}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "            \n",
    "    print(f\"-> Found {len(documents)} guide files.\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "# Execute the parsing functions\n",
    "guide_docs = parse_guides(DOCS_PATH, \"sleap\", repo_path=REPO_PATH)\n",
    "api_docs = parse_source_code(SRC_PATH, \"sleap\", repo_path=REPO_PATH)\n",
    "\n",
    "# Combine the results into a single list\n",
    "all_raw_docs = guide_docs + api_docs\n",
    "\n",
    "print(f\"\\n✅ Total documents collected: {len(all_raw_docs)}\")\n",
    "\n",
    "# print 2 examples each\n",
    "print(\"\\nExample documents:\")\n",
    "for doc in guide_docs[:2]:\n",
    "    print(f\"Source: {doc.metadata['source']}, File: {doc.metadata.get('file', 'N/A')}\")\n",
    "    print(doc.page_content[:100] + \"...\")  # Print first 100 characters\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "for doc in api_docs[:2]:\n",
    "    print(f\"Source: {doc.metadata['source']}, File: {doc.metadata.get('file', 'N/A')}, Object: {doc.metadata.get('object', 'N/A')}\")\n",
    "    print(doc.page_content[:100] + \"...\\n\")  # Print first 100 characters\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d31759d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing guides in: /Users/chan/PersonalProjects/rag-sleap-docs/sleap-io/docs...\n",
      "-> Found 4 guide files.\n",
      "Parsing source code in: /Users/chan/PersonalProjects/rag-sleap-docs/sleap-io/sleap_io...\n",
      "-> Found 398 docstrings in source code.\n",
      "Parsing guides in: /Users/chan/PersonalProjects/rag-sleap-docs/dreem/docs...\n",
      "-> Found 24 guide files.\n",
      "Parsing source code in: /Users/chan/PersonalProjects/rag-sleap-docs/dreem/dreem...\n",
      "-> Found 345 docstrings in source code.\n"
     ]
    }
   ],
   "source": [
    "# Now we do sleap-io docs\n",
    "\n",
    "SLEAPIO_REPO_PATH = Path(\"/Users/chan/PersonalProjects/rag-sleap-docs/sleap-io\") \n",
    "SLEAPIO_DOCS_PATH = SLEAPIO_REPO_PATH / \"docs\"\n",
    "SLEAPIO_SRC_PATH = SLEAPIO_REPO_PATH / \"sleap_io\"\n",
    "\n",
    "\n",
    "# Execute the parsing functions\n",
    "sio_guide_docs = parse_guides(SLEAPIO_DOCS_PATH, \"sleap-io\", repo_path=SLEAPIO_REPO_PATH)\n",
    "sio_api_docs = parse_source_code(SLEAPIO_SRC_PATH, \"sleap-io\", repo_path=SLEAPIO_REPO_PATH)\n",
    "\n",
    "# Combine the results into a single list\n",
    "sio_all_raw_docs = sio_guide_docs + sio_api_docs\n",
    "\n",
    "DREEM_REPO_PATH = Path(\"/Users/chan/PersonalProjects/rag-sleap-docs/dreem\")\n",
    "DREEM_DOCS_PATH = DREEM_REPO_PATH / \"docs\"\n",
    "DREEM_SRC_PATH = DREEM_REPO_PATH / \"dreem\"\n",
    "\n",
    "dreem_guide_docs = parse_guides(DREEM_DOCS_PATH, \"dreem\", repo_path=DREEM_REPO_PATH)\n",
    "dreem_api_docs = parse_source_code(DREEM_SRC_PATH, \"dreem\", repo_path=DREEM_REPO_PATH)\n",
    "\n",
    "# Combine the results into a single list\n",
    "dreem_all_raw_docs = dreem_guide_docs + dreem_api_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0912c6",
   "metadata": {},
   "source": [
    "## Now, to split everything: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1684cee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents processed: 2874\n",
      "Total chunks created: 3562\n",
      "\n",
      "Sample chunk metadata from a guide:\n",
      "{'H1': 'Contributor Covenant Code of Conduct', 'H2': 'Our Pledge', 'source': 'sleap-guide', 'file': 'docs/CODE_OF_CONDUCT.md'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language, MarkdownHeaderTextSplitter\n",
    "\n",
    "# Combine your raw document lists\n",
    "all_docs_combined = all_raw_docs + sio_all_raw_docs + dreem_all_raw_docs\n",
    "\n",
    "# --- 1. Define your splitters ---\n",
    "\n",
    "# For splitting guides by their headers\n",
    "headers_to_split_on = [(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\")]\n",
    "md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "\n",
    "# For splitting very long docstrings OR language-specific files\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "# This is the correct way to handle RST files\n",
    "rst_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.RST, chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "\n",
    "# --- 2. Process documents based on type ---\n",
    "\n",
    "final_chunks = []\n",
    "\n",
    "for doc in all_docs_combined:\n",
    "    source_type = doc.metadata.get(\"source\", \"\")\n",
    "    file_path = doc.metadata.get(\"file\", \"\")\n",
    "\n",
    "    # A. Process structured guides\n",
    "    if \"guide\" in source_type:\n",
    "        if file_path.endswith(\".md\"):\n",
    "            chunks = md_splitter.split_text(doc.page_content)\n",
    "            # Add original metadata back to the new chunks\n",
    "            for chunk in chunks:\n",
    "                chunk.metadata.update(doc.metadata)\n",
    "            final_chunks.extend(chunks)\n",
    "\n",
    "        elif file_path.endswith(\".rst\"):\n",
    "            # Use the language-specific splitter for RST\n",
    "            chunks = rst_splitter.split_documents([doc])\n",
    "            final_chunks.extend(chunks)\n",
    "\n",
    "    # B. Process code docstrings\n",
    "    elif \"api\" in source_type:\n",
    "        # If the docstring is larger than our chunk size, split it\n",
    "        if len(doc.page_content) > chunk_size:\n",
    "            chunks = recursive_splitter.split_documents([doc])\n",
    "            final_chunks.extend(chunks)\n",
    "        else:\n",
    "            # Otherwise, keep it as a single, logical chunk\n",
    "            final_chunks.append(doc)\n",
    "\n",
    "# --- 3. Verification ---\n",
    "print(f\"Total documents processed: {len(all_docs_combined)}\")\n",
    "print(f\"Total chunks created: {len(final_chunks)}\")\n",
    "print(\"\\nSample chunk metadata from a guide:\")\n",
    "# Find and print a sample guide chunk if it exists\n",
    "for chunk in final_chunks:\n",
    "    if \"guide\" in chunk.metadata.get(\"source\", \"\"):\n",
    "        print(chunk.metadata)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea23fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.13/site-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding SLEAP documents...\n"
     ]
    }
   ],
   "source": [
    "# Now I embed the documents and add them to the ChromaDB collections\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "embeddings = VertexAIEmbeddings(\n",
    "    model_name=\"text-embedding-004\"\n",
    ")\n",
    "\n",
    "# Check if collections already have data\n",
    "sleap_collection = client.get_collection(\"sleap\")\n",
    "\n",
    "if sleap_collection.count() > 0:\n",
    "    print(f\"SLEAP collection already has {sleap_collection.count()} documents. Skipping embedding.\")\n",
    "    sleap_vectorstore = Chroma(\n",
    "        client=client,\n",
    "        collection_name=\"sleap\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "else:\n",
    "    print(\"Embedding SLEAP documents...\")\n",
    "    sleap_vectorstore = Chroma.from_documents(\n",
    "        final_chunks,\n",
    "        embeddings,\n",
    "        collection_name=\"sleap\",\n",
    "        client=client,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11ada255",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleap_retriever = sleap_vectorstore.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f9f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test this out\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful AI assistant specialized in SLEAP (Social LEAP Estimates Animal Poses), SLEAP-IO, and DREEM documentation.\n",
    "\n",
    "Use the following context from the documentation to answer the user's question. If the answer cannot be found in the context, say \"I don't have enough information in the provided documentation to answer that question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Provide accurate, detailed answers based on the documentation\n",
    "- Include code examples when relevant\n",
    "- Mention specific function names, classes, or modules when applicable\n",
    "- If discussing installation or setup, be specific about requirements\n",
    "- For troubleshooting questions, provide step-by-step solutions\n",
    "- Always cite which part of the documentation (SLEAP, SLEAP-IO, or DREEM) your answer comes from\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-2.0-flash-lite\",\n",
    "    temperature=0.2)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": sleap_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"How do I use DREEM on a existing SLEAP prediction?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Create memory to store conversation history\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "# Updated prompt template that includes chat history\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful AI assistant specialized in SLEAP (Social LEAP Estimates Animal Poses), SLEAP-IO, and DREEM documentation.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Use the following context from the documentation to answer the user's question. If the answer cannot be found in the context, say \"I don't have enough information in the provided documentation to answer that question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Current Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Provide accurate, detailed answers based on the documentation\n",
    "- Include code examples when relevant\n",
    "- Mention specific function names, classes, or modules when applicable\n",
    "- If discussing installation or setup, be specific about requirements\n",
    "- For troubleshooting questions, provide step-by-step solutions\n",
    "- Always cite which part of the documentation (SLEAP, SLEAP-IO, or DREEM) your answer comes from\n",
    "- Reference previous conversation when relevant\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-2.0-flash-lite\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Function to get chat history\n",
    "def get_chat_history(_):\n",
    "    return memory.chat_memory.messages\n",
    "\n",
    "# Conversational RAG Chain\n",
    "conversational_rag_chain = (\n",
    "    {\n",
    "        \"context\": sleap_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"chat_history\": RunnableLambda(get_chat_history),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Function to chat with memory\n",
    "def chat_with_memory(question: str) -> str:\n",
    "    # Get the response\n",
    "    response = conversational_rag_chain.invoke(question)\n",
    "    \n",
    "    # Save to memory\n",
    "    memory.save_context({\"question\": question}, {\"answer\": response})\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the conversational chain\n",
    "print(\"=== Conversation 1 ===\")\n",
    "response1 = chat_with_memory(\"How do I install SLEAP?\")\n",
    "print(response1)\n",
    "\n",
    "print(\"\\n=== Conversation 2 ===\")\n",
    "response2 = chat_with_memory(\"What about the GPU requirements?\")\n",
    "print(response2)\n",
    "\n",
    "print(\"\\n=== Conversation 3 ===\")\n",
    "response3 = chat_with_memory(\"Can you show me a code example for the installation process we discussed?\")\n",
    "print(response3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
