{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "021d6a44",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "I need to index everything for sleap and sleap-io at first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".local.env\")\n",
    "os.environ[\"LANGSMITH_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = dotenv.get_key(\".local.env\", \"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"rag-sleap-docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ec43a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import langchain_community\n",
    "import langchain_google_vertexai\n",
    "import chromadb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222008b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: at this point I should be able to see the LangSmith configuration\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "# gemini = ChatVertexAI(\n",
    "#     model_name=\"gemini-2.0-flash-lite\",\n",
    "#     temperature=0.2)\n",
    "# gemini.invoke(\"Repeat after me: 'Hello, world!'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fed1e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections created or accessed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Make or bind a ChromaDB client\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "try:\n",
    "    sleap_collection = client.get_or_create_collection(name=\"sleap\")\n",
    "    sleap_io_collection = client.get_or_create_collection(name=\"sleap_io\")\n",
    "    print(\"Collections created or accessed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating or accessing collection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e019479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "SLEAP_DOCS_URL = \"./sleap_docs\"\n",
    "SLEAP_IO_DOCS_URL = \"./sleap_io_docs/0.4.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6640fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleap_loader = DirectoryLoader(\n",
    "    SLEAP_DOCS_URL,\n",
    "    glob=\"**/*.html\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "\n",
    "sleap_io_loader = DirectoryLoader(\n",
    "    SLEAP_IO_DOCS_URL,\n",
    "    glob=\"**/*.html\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c5fb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleap_docs = sleap_loader.load()\n",
    "sleap_io_docs = sleap_io_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e5e07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding: sleap_docs/py-modindex.html\n",
      "Excluding: sleap_docs/genindex.html\n",
      "Excluding: sleap_docs/search.html\n",
      "Excluding: sleap_docs/develop/help.html\n",
      "Excluding: sleap_docs/develop/index.html\n",
      "Excluding: sleap_docs/develop/py-modindex.html\n",
      "Excluding: sleap_docs/develop/CODE_OF_CONDUCT.html\n",
      "Excluding: sleap_docs/develop/overview.html\n",
      "Excluding: sleap_docs/develop/datasets.html\n",
      "Excluding: sleap_docs/develop/genindex.html\n",
      "Excluding: sleap_docs/develop/CONTRIBUTING.html\n",
      "Excluding: sleap_docs/develop/search.html\n",
      "Excluding: sleap_docs/develop/api.html\n",
      "Excluding: sleap_docs/develop/installation.html\n",
      "Excluding: sleap_docs/main/py-modindex.html\n",
      "Excluding: sleap_docs/main/genindex.html\n",
      "Excluding: sleap_docs/main/search.html\n",
      "Excluding: sleap_docs/develop/_static/webpack-macros.html\n",
      "Excluding: sleap_docs/develop/_static/sbt-webpack-macros.html\n",
      "Excluding: sleap_docs/develop/guides/merging.html\n",
      "Excluding: sleap_docs/develop/guides/choosing-models.html\n",
      "Excluding: sleap_docs/develop/guides/index.html\n",
      "Excluding: sleap_docs/develop/guides/troubleshooting-workflows.html\n",
      "Excluding: sleap_docs/develop/guides/proofreading.html\n",
      "Excluding: sleap_docs/develop/guides/remote.html\n",
      "Excluding: sleap_docs/develop/guides/skeletons.html\n",
      "Excluding: sleap_docs/develop/guides/bonsai.html\n",
      "Excluding: sleap_docs/develop/guides/cli.html\n",
      "Excluding: sleap_docs/develop/guides/colab.html\n",
      "Excluding: sleap_docs/develop/guides/custom-training.html\n",
      "Excluding: sleap_docs/develop/guides/training.html\n",
      "Excluding: sleap_docs/develop/guides/gui.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.genericjson.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.model.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.deeplabcut.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.peak_finding.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.config.utils.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.main.html\n",
      "Excluding: sleap_docs/develop/api/sleap.info.write_tracking_h5.html\n",
      "Excluding: sleap_docs/develop/api/sleap.message.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.augmentation.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.labels_json.html\n",
      "Excluding: sleap_docs/develop/api/sleap.skeleton.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.filehandle.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.dataset_ops.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.adaptor.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.offset_regression.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.paf_grouping.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.general.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.legacy.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.csv.html\n",
      "Excluding: sleap_docs/develop/api/sleap.info.metrics.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.grouping.html\n",
      "Excluding: sleap_docs/develop/api/sleap.info.summary.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.training.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.evals.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.architectures.hourglass.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.edge_maps.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.nix.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.tracker.components.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.heads.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.architectures.encoder_decoder.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.coco.html\n",
      "Excluding: sleap_docs/develop/api/sleap.util.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.architectures.leap.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.alphatracker.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.normalization.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.dataset.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.pathutils.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.inference.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.training.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.config.optimization.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.viz.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.dispatch.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.ndx_pose.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.config.data.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.convert.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.pipelines.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.tracking.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.hdf5.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.instance_cropping.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.inference.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.system.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.architectures.common.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.deepposekit.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.text.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.utils.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.tracker.kalman.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.leap_matlab.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.providers.html\n",
      "Excluding: sleap_docs/develop/api/sleap.instance.html\n",
      "Excluding: sleap_docs/develop/api/sleap.info.labels.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.config.model.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.identity.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.visuals.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.architectures.resnet.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.losses.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.identity.html\n",
      "Excluding: sleap_docs/develop/api/sleap.info.align.html\n",
      "Excluding: sleap_docs/develop/api/sleap.info.feature_suggestions.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.videowriter.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.utils.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.config.outputs.html\n",
      "Excluding: sleap_docs/develop/api/sleap.info.trackcleaner.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.instance_centroids.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.architectures.upsampling.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.format.sleap_analysis.html\n",
      "Excluding: sleap_docs/develop/api/sleap.io.video.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.architectures.unet.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.confidence_maps.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.data.resizing.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.architectures.hrnet.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.config.training_job.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.callbacks.html\n",
      "Excluding: sleap_docs/develop/api/sleap.nn.architectures.pretrained_encoders.html\n",
      "Excluding: sleap_docs/develop/tutorials/initial-labeling.html\n",
      "Excluding: sleap_docs/develop/tutorials/new-project.html\n",
      "Excluding: sleap_docs/develop/tutorials/tutorial.html\n",
      "Excluding: sleap_docs/develop/tutorials/initial-training.html\n",
      "Excluding: sleap_docs/develop/tutorials/analysis.html\n",
      "Excluding: sleap_docs/develop/tutorials/proofreading.html\n",
      "Excluding: sleap_docs/develop/tutorials/assisted-labeling.html\n",
      "Excluding: sleap_docs/develop/notebooks/Post_inference_tracking.html\n",
      "Excluding: sleap_docs/develop/notebooks/Training_and_inference_using_Google_Drive.html\n",
      "Excluding: sleap_docs/develop/notebooks/index.html\n",
      "Excluding: sleap_docs/develop/notebooks/Data_structures.html\n",
      "Excluding: sleap_docs/develop/notebooks/Analysis_examples.html\n",
      "Excluding: sleap_docs/develop/notebooks/Interactive_and_resumable_training.html\n",
      "Excluding: sleap_docs/develop/notebooks/Interactive_and_realtime_inference.html\n",
      "Excluding: sleap_docs/develop/notebooks/Model_evaluation.html\n",
      "Excluding: sleap_docs/develop/notebooks/Training_and_inference_on_an_example_dataset.html\n",
      "Excluding: sleap_docs/develop/notebooks/analysis_example/README.html\n",
      "Filtered 629 -> 497 documents\n",
      "Filtered 27 -> 27 documents\n"
     ]
    }
   ],
   "source": [
    "# Filter out unwanted documents\n",
    "def filter_docs(docs):\n",
    "    filtered = []\n",
    "    exclude_patterns = [\"/develop/\", \"genindex.html\", \"modindex.html\", \"search.html\"]\n",
    "    \n",
    "    for doc in docs:\n",
    "        source_path = doc.metadata.get('source', '')\n",
    "        if not any(pattern in source_path for pattern in exclude_patterns):\n",
    "            filtered.append(doc)\n",
    "        else:\n",
    "            print(f\"Excluding: {source_path}\")\n",
    "    \n",
    "    print(f\"Filtered {len(docs)} -> {len(filtered)} documents\")\n",
    "    return filtered\n",
    "\n",
    "sleap_docs = filter_docs(sleap_docs)\n",
    "sleap_io_docs = filter_docs(sleap_io_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7f3d663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_main_content_selector(html_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Invokes an LLM to find the best CSS selector for the main content\n",
    "    of a given HTML document. This is the \"expert\" we call when needed.\n",
    "    \"\"\"\n",
    "    print(\"\\nAsking LLM to find the best selector for a specific page...\")\n",
    "    \n",
    "    # Initialize the LLM\n",
    "    llm = ChatVertexAI(model_name=\"gemini-2.0-flash-lite\", temperature=0)\n",
    "    \n",
    "    # Define the prompt for the LLM\n",
    "    prompt_template = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Analyze the following HTML document and identify the single best CSS selector \n",
    "that precisely targets the main article or content area.\n",
    "Ignore headers, footers, navigation bars, and sidebars.\n",
    "Respond with ONLY the CSS selector string and nothing else. Do NOT include any additional text or explanations. Do NOT include a markdown code block.\n",
    "\n",
    "HTML:\n",
    "{html_content}\n",
    "\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Define the chain to get the selector\n",
    "    chain = prompt_template | llm | StrOutputParser()\n",
    "    \n",
    "    # Invoke the chain\n",
    "    selector = chain.invoke({\"html_content\": html_content})\n",
    "    \n",
    "    print(f\"‚úÖ LLM identified new selector: '{selector.strip()}'\")\n",
    "    return selector.strip()\n",
    "\n",
    "def transform_docs_with_selector(docs: list[Document], selectors: list[str]) -> list[Document]:\n",
    "    \"\"\"\n",
    "    Applies a list of CSS selectors to a list of documents.\n",
    "    If no selectors work for a document, it calls an LLM to find a new one\n",
    "    and adds it to the list for future use.\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying selectors to all {len(docs)} documents...\")\n",
    "    transformed_docs = []\n",
    "    selector_counts = {}  # Track usage of each selector\n",
    "\n",
    "    for doc in tqdm(docs, desc=\"Processing documents\", unit=\"doc\"):\n",
    "        soup = BeautifulSoup(doc.page_content, \"html.parser\")\n",
    "        \n",
    "        clean_content = \"\"\n",
    "        content_found = False\n",
    "        used_selector = None\n",
    "        \n",
    "        # Try all existing selectors first\n",
    "        for selector in selectors:\n",
    "            main_content_element = soup.select_one(selector)\n",
    "            if main_content_element:\n",
    "                clean_content = main_content_element.get_text(separator=' ', strip=True)\n",
    "                content_found = True\n",
    "                used_selector = selector\n",
    "                break # Selector worked, move to the next document\n",
    "        \n",
    "        # If no existing selectors worked, call the LLM for help\n",
    "        if not content_found:\n",
    "            print(f\"    ‚ö†Ô∏è No existing selectors worked for {doc.metadata['source']}. Finding a new one...\")\n",
    "            new_selector = get_main_content_selector(doc.page_content)\n",
    "            selectors.append(new_selector) # Add the new selector to our list\n",
    "            used_selector = new_selector\n",
    "            \n",
    "            # Try again with the new selector\n",
    "            main_content_element = soup.select_one(new_selector)\n",
    "            if main_content_element:\n",
    "                clean_content = main_content_element.get_text(separator=' ', strip=True)\n",
    "            else:\n",
    "                clean_content = f\"[Content not found even with new selector '{new_selector}']\"\n",
    "\n",
    "        # Track selector usage\n",
    "        if used_selector:\n",
    "            selector_counts[used_selector] = selector_counts.get(used_selector, 0) + 1\n",
    "\n",
    "        transformed_docs.append(Document(page_content=clean_content, metadata=doc.metadata))\n",
    "\n",
    "    # Print selector usage statistics\n",
    "    print(\"\\nüìä Selector Usage Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "    for selector, count in sorted(selector_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (count / len(docs)) * 100\n",
    "        print(f\"'{selector}': {count} docs ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nTotal selectors used: {len(selector_counts)}\")\n",
    "    print(f\"Total documents processed: {len(docs)}\")\n",
    "        \n",
    "    return transformed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f183b628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing SLEAP docs with LLM selector detection ===\n",
      "\n",
      "Asking LLM to find the best selector for a specific page...\n",
      "‚úÖ LLM identified new selector: '.body'\n",
      "\n",
      "Applying selectors to all 497 documents...\n",
      "‚úÖ LLM identified new selector: '.body'\n",
      "\n",
      "Applying selectors to all 497 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   0%|          | 0/497 [00:00<?, ?doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚ö†Ô∏è No existing selectors worked for sleap_docs/help.html. Finding a new one...\n",
      "\n",
      "Asking LLM to find the best selector for a specific page...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   1%|          | 3/497 [00:00<01:44,  4.72doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM identified new selector: 'main-content'\n",
      "    ‚ö†Ô∏è No existing selectors worked for sleap_docs/index.html. Finding a new one...\n",
      "\n",
      "Asking LLM to find the best selector for a specific page...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   1%|          | 4/497 [00:01<02:46,  2.97doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM identified new selector: 'main-content'\n",
      "    ‚ö†Ô∏è No existing selectors worked for sleap_docs/CODE_OF_CONDUCT.html. Finding a new one...\n",
      "\n",
      "Asking LLM to find the best selector for a specific page...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   2%|‚ñè         | 9/497 [00:01<01:32,  5.27doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM identified new selector: 'article.bd-article'\n",
      "    ‚ö†Ô∏è No existing selectors worked for sleap_docs/includeme.html. Finding a new one...\n",
      "\n",
      "Asking LLM to find the best selector for a specific page...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   3%|‚ñé         | 16/497 [00:02<01:05,  7.34doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM identified new selector: 'div.wy-nav-content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   4%|‚ñç         | 22/497 [00:02<00:37, 12.79doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚ö†Ô∏è No existing selectors worked for sleap_docs/_static/webpack-macros.html. Finding a new one...\n",
      "\n",
      "Asking LLM to find the best selector for a specific page...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   5%|‚ñå         | 25/497 [00:03<00:48,  9.76doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM identified new selector: 'main'\n",
      "    ‚ö†Ô∏è No existing selectors worked for sleap_docs/_static/sbt-webpack-macros.html. Finding a new one...\n",
      "\n",
      "Asking LLM to find the best selector for a specific page...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents:   9%|‚ñâ         | 46/497 [00:03<00:18, 24.37doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM identified new selector: 'main'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 497/497 [00:11<00:00, 42.24doc/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Selector Usage Statistics:\n",
      "--------------------------------------------------\n",
      "'.body': 258 docs (51.9%)\n",
      "'article.bd-article': 120 docs (24.1%)\n",
      "'main': 113 docs (22.7%)\n",
      "'div.wy-nav-content': 4 docs (0.8%)\n",
      "'main-content': 2 docs (0.4%)\n",
      "\n",
      "Total selectors used: 5\n",
      "Total documents processed: 497\n",
      "\n",
      "=== Processing SLEAP-IO docs with LLM selector detection ===\n",
      "\n",
      "Asking LLM to find the best selector for a specific page...\n",
      "‚úÖ LLM identified new selector: '.md-content'\n",
      "\n",
      "Applying selectors to all 27 documents...\n",
      "‚úÖ LLM identified new selector: '.md-content'\n",
      "\n",
      "Applying selectors to all 27 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:02<00:00, 11.32doc/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Selector Usage Statistics:\n",
      "--------------------------------------------------\n",
      "'.md-content': 27 docs (100.0%)\n",
      "\n",
      "Total selectors used: 1\n",
      "Total documents processed: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Transform raw HTML documents using LLM-based selector detection\n",
    "print(\"\\n=== Processing SLEAP docs with LLM selector detection ===\")\n",
    "if sleap_docs:\n",
    "    # Get the selector by analyzing the FIRST document\n",
    "    sample_html = sleap_docs[0].page_content\n",
    "    sleap_selector = get_main_content_selector(sample_html)\n",
    "    \n",
    "    # Apply that selector to ALL sleap documents\n",
    "    sleap_docs = transform_docs_with_selector(sleap_docs, [sleap_selector])\n",
    "\n",
    "print(\"\\n=== Processing SLEAP-IO docs with LLM selector detection ===\")\n",
    "if sleap_io_docs:\n",
    "    # Get the selector by analyzing the FIRST document  \n",
    "    sample_html = sleap_io_docs[0].page_content\n",
    "    sleap_io_selector = get_main_content_selector(sample_html)\n",
    "    \n",
    "    # Apply that selector to ALL sleap-io documents\n",
    "    sleap_io_docs = transform_docs_with_selector(sleap_io_docs, [sleap_io_selector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c60ab20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Adjust chunk size as needed\n",
    "    chunk_overlap=200,  # Adjust overlap as needed\n",
    "    length_function=len)\n",
    "sleap_splits = text_splitter.split_documents(sleap_docs)\n",
    "sleap_io_splits = text_splitter.split_documents(sleap_io_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea23fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.12/site-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding SLEAP documents...\n",
      "Embedding SLEAP-IO documents...\n",
      "Embedding SLEAP-IO documents...\n"
     ]
    }
   ],
   "source": [
    "# Now I embed the documents and add them to the ChromaDB collections\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "embeddings = VertexAIEmbeddings(\n",
    "    model_name=\"text-embedding-004\"\n",
    ")\n",
    "\n",
    "# Check if collections already have data\n",
    "sleap_collection = client.get_collection(\"sleap\")\n",
    "sleap_io_collection = client.get_collection(\"sleap_io\")\n",
    "\n",
    "if sleap_collection.count() > 0:\n",
    "    print(f\"SLEAP collection already has {sleap_collection.count()} documents. Skipping embedding.\")\n",
    "    sleap_vectorstore = Chroma(\n",
    "        client=client,\n",
    "        collection_name=\"sleap\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "else:\n",
    "    print(\"Embedding SLEAP documents...\")\n",
    "    sleap_vectorstore = Chroma.from_documents(\n",
    "        sleap_splits,\n",
    "        embeddings,\n",
    "        collection_name=\"sleap\",\n",
    "        client=client,\n",
    "    )\n",
    "\n",
    "if sleap_io_collection.count() > 0:\n",
    "    print(f\"SLEAP-IO collection already has {sleap_io_collection.count()} documents. Skipping embedding.\")\n",
    "    sleap_io_vectorstore = Chroma(\n",
    "        client=client,\n",
    "        collection_name=\"sleap_io\",\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "else:\n",
    "    print(\"Embedding SLEAP-IO documents...\")\n",
    "    sleap_io_vectorstore = Chroma.from_documents(\n",
    "        sleap_io_splits,\n",
    "        embeddings,\n",
    "        collection_name=\"sleap_io\",\n",
    "        client=client,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11ada255",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleap_retriever = sleap_vectorstore.as_retriever()\n",
    "sleap_io_retriever = sleap_io_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad0f9f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can fine-tune an existing model with new data by importing data from a predictions file into your SLEAP project, correcting the predictions, and then training a new model.\\n\\nHere\\'s how:\\n\\n1.  **Get predictions:** Use your trained model to generate predictions on a new video.\\n2.  **Open predictions file in GUI:** Open the predictions file in the SLEAP GUI.\\n3.  **Import data:** Import the data from the predictions file into the SLEAP project that contains your original training data.\\n4.  **Correct predictions:** Make corrections to the predictions in the GUI.\\n5.  **Train a new model:** Train a new model using the corrected data. The new model will be trained from scratch using the corrections. It will not be trained on the original data used to train the previous models.\\n\\nThis information comes from the SLEAP documentation section \"Add more training data to a project\".\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test this out\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful AI assistant specialized in SLEAP (Social LEAP Estimates Animal Poses) and SLEAP-IO documentation.\n",
    "\n",
    "Use the following context from the documentation to answer the user's question. If the answer cannot be found in the context, say \"I don't have enough information in the provided documentation to answer that question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Provide accurate, detailed answers based on the documentation\n",
    "- Include code examples when relevant\n",
    "- Mention specific function names, classes, or modules when applicable\n",
    "- If discussing installation or setup, be specific about requirements\n",
    "- For troubleshooting questions, provide step-by-step solutions\n",
    "- Always cite which part of the documentation (SLEAP or SLEAP-IO) your answer comes from\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"gemini-2.0-flash-lite\",\n",
    "    temperature=0.2)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": sleap_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"How do I fine-tune an existing model with new data in SLEAP?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b96e37f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
